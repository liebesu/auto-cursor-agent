#!/usr/bin/env python3
"""
Auto Cursor Agent é›†æˆæµ‹è¯•

æµ‹è¯•å®Œæ•´çš„å·¥ä½œæµç¨‹å’Œå„æ¨¡å—ä¹‹é—´çš„é›†æˆ
"""

import asyncio
import tempfile
# import pytest  # æ³¨é‡Šæ‰pytestä¾èµ–ï¼Œä½¿ç”¨åŸºç¡€æµ‹è¯•
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from main import AutoCursorAgent
from utils.config_manager import ConfigManager


class TestIntegration:
    """é›†æˆæµ‹è¯•ç±»"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
        self.sample_requirements = [
            "åˆ›å»ºä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨åº”ç”¨",
            "å¼€å‘ä¸€ä¸ªtodoä»»åŠ¡ç®¡ç†å·¥å…·", 
            "åˆ¶ä½œä¸€ä¸ªå¤©æ°”æŸ¥è¯¢APIæœåŠ¡"
        ]
    
    def test_config_loading(self):
        """æµ‹è¯•é…ç½®åŠ è½½"""
        config = self.config_manager.get_config()
        
        assert isinstance(config, dict)
        assert 'ai_models' in config
        assert 'cursor' in config
        assert 'monitoring' in config
    
    async def test_need_analysis_workflow(self):
        """æµ‹è¯•éœ€æ±‚åˆ†æå·¥ä½œæµç¨‹"""
        
        from core.need_analyzer import NeedAnalyzer
        
        config = self.config_manager.get_config()
        analyzer = NeedAnalyzer(config)
        
        for requirement in self.sample_requirements:
            try:
                # åˆ†æéœ€æ±‚
                result = await analyzer.analyze(requirement)
                
                # éªŒè¯åŸºæœ¬ç»“æ„
                assert 'original_requirement' in result
                assert 'project_type' in result
                assert 'features' in result
                assert 'tech_stack' in result
                assert 'complexity' in result
                
                # éªŒè¯æ•°æ®ç±»å‹
                assert isinstance(result['features'], list)
                assert isinstance(result['tech_stack'], dict)
                assert result['complexity'] in ['low', 'medium', 'high']
                
                print(f"âœ… éœ€æ±‚åˆ†ææµ‹è¯•é€šè¿‡: {requirement[:30]}...")
                
            except Exception as e:
                print(f"âŒ éœ€æ±‚åˆ†ææµ‹è¯•å¤±è´¥: {e}")
                # å¯¹äºAI APIä¸å¯ç”¨çš„æƒ…å†µï¼Œä½¿ç”¨fallbackæµ‹è¯•
                assert 'fallback_analysis' in str(e) or 'API' in str(e)
    
    @pytest.mark.asyncio
    async def test_task_decomposition_workflow(self, config_manager):
        """æµ‹è¯•ä»»åŠ¡åˆ†è§£å·¥ä½œæµç¨‹"""
        
        from core.task_orchestrator import TaskOrchestrator
        
        config = config_manager.get_config()
        orchestrator = TaskOrchestrator(config)
        
        # æ¨¡æ‹Ÿéœ€æ±‚åˆ†æç»“æœ
        mock_requirement = {
            'project_type': 'web_app',
            'complexity': 'medium',
            'features': [
                {
                    'name': 'ç”¨æˆ·ç™»å½•',
                    'description': 'ç”¨æˆ·å¯ä»¥ç™»å½•ç³»ç»Ÿ',
                    'priority': 4,
                    'estimated_hours': 3
                },
                {
                    'name': 'æ•°æ®å±•ç¤º',
                    'description': 'æ˜¾ç¤ºç”¨æˆ·æ•°æ®',
                    'priority': 3,
                    'estimated_hours': 2
                }
            ],
            'tech_stack': {
                'frontend': ['React'],
                'backend': ['Node.js']
            }
        }
        
        try:
            # åˆ†è§£ä»»åŠ¡
            tasks = await orchestrator.decompose_tasks(mock_requirement)
            
            # éªŒè¯ä»»åŠ¡ç»“æ„
            assert isinstance(tasks, list)
            assert len(tasks) > 0
            
            for task in tasks:
                assert 'id' in task
                assert 'name' in task
                assert 'type' in task
                assert 'status' in task
                assert 'estimated_hours' in task
                
                # éªŒè¯ä¾èµ–å…³ç³»
                if 'dependencies' in task:
                    assert isinstance(task['dependencies'], list)
            
            print(f"âœ… ä»»åŠ¡åˆ†è§£æµ‹è¯•é€šè¿‡: ç”Ÿæˆäº† {len(tasks)} ä¸ªä»»åŠ¡")
            
        except Exception as e:
            print(f"âŒ ä»»åŠ¡åˆ†è§£æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    @pytest.mark.asyncio
    async def test_progress_monitoring(self, config_manager, test_workspace):
        """æµ‹è¯•è¿›åº¦ç›‘æ§åŠŸèƒ½"""
        
        from core.progress_monitor import ProgressMonitor
        
        config = config_manager.get_config()
        monitor = ProgressMonitor(config)
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        workspace = Path(test_workspace)
        test_file = workspace / "test.py"
        test_file.write_text("print('Hello, World!')")
        
        # å¯åŠ¨ç›‘æ§
        monitor.start_monitoring(test_workspace)
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
        await asyncio.sleep(1)
        
        # è·å–è¿›åº¦
        progress = monitor.get_progress()
        
        # éªŒè¯è¿›åº¦ç»“æ„
        assert isinstance(progress, dict)
        if progress.get('status') == 'monitoring':
            assert 'completion_rate' in progress
            assert 'quality_score' in progress
            assert 'files_created' in progress
        
        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring()
        
        print("âœ… è¿›åº¦ç›‘æ§æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_delivery_validation(self, config_manager, test_workspace):
        """æµ‹è¯•äº¤ä»˜éªŒè¯åŠŸèƒ½"""
        
        from core.delivery_manager import DeliveryManager, ProjectValidator
        
        # åˆ›å»ºåŸºæœ¬é¡¹ç›®ç»“æ„
        workspace = Path(test_workspace)
        (workspace / "src").mkdir()
        (workspace / "README.md").write_text("# Test Project")
        (workspace / "package.json").write_text('{"name": "test"}')
        
        # æ¨¡æ‹Ÿéœ€æ±‚å’Œä»»åŠ¡
        mock_requirements = {
            'project_type': 'web_app',
            'features': [{'name': 'basic feature'}]
        }
        
        mock_tasks = [
            {'id': 'task1', 'name': 'Setup', 'status': 'completed'},
            {'id': 'task2', 'name': 'Development', 'status': 'completed'}
        ]
        
        # éªŒè¯é¡¹ç›®
        validator = ProjectValidator()
        validation_result = await validator.validate_project(
            test_workspace, mock_requirements, mock_tasks
        )
        
        # éªŒè¯ç»“æœç»“æ„
        assert 'overall_status' in validation_result
        assert 'overall_score' in validation_result
        assert 'validations' in validation_result
        
        print(f"âœ… é¡¹ç›®éªŒè¯æµ‹è¯•é€šè¿‡: çŠ¶æ€ {validation_result['overall_status']}")
    
    @pytest.mark.asyncio
    async def test_end_to_end_workflow(self, config_manager, test_workspace):
        """ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æµ‹è¯•"""
        
        config = config_manager.get_config()
        
        # ä½¿ç”¨ç®€åŒ–çš„é…ç½®é¿å…APIè°ƒç”¨
        test_config = config.copy()
        test_config['ai_models'] = {
            'openai': {'api_key': 'test_key'},
            'claude': {'api_key': 'test_key'}
        }
        
        try:
            # åˆ›å»ºä»£ç†å®ä¾‹
            agent = AutoCursorAgent()
            
            # ç®€å•éœ€æ±‚
            test_requirement = "åˆ›å»ºä¸€ä¸ªç®€å•çš„Hello Worldåº”ç”¨"
            
            # ç”±äºéœ€è¦çœŸå®çš„AI APIï¼Œè¿™é‡Œåªæµ‹è¯•åˆå§‹åŒ–
            assert agent.need_analyzer is not None
            assert agent.task_orchestrator is not None
            assert agent.cursor_interface is not None
            assert agent.progress_monitor is not None
            assert agent.auto_optimizer is not None
            assert agent.delivery_manager is not None
            
            print("âœ… ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âš ï¸  ç«¯åˆ°ç«¯æµ‹è¯•å—é™äºAPIå¯ç”¨æ€§: {e}")
            # è¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰çœŸå®çš„APIå¯†é’¥
    
    def test_configuration_validation(self, config_manager):
        """æµ‹è¯•é…ç½®éªŒè¯"""
        
        config = config_manager.get_config()
        
        # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        required_sections = ['ai_models', 'cursor', 'monitoring']
        for section in required_sections:
            assert section in config, f"ç¼ºå°‘é…ç½®èŠ‚: {section}"
        
        # éªŒè¯AIæ¨¡å‹é…ç½®
        ai_models = config['ai_models']
        assert isinstance(ai_models, dict)
        
        # éªŒè¯Cursoré…ç½®
        cursor_config = config['cursor']
        assert isinstance(cursor_config, dict)
        assert 'executable_path' in cursor_config
        
        print("âœ… é…ç½®éªŒè¯æµ‹è¯•é€šè¿‡")
    
    def test_error_handling(self, config_manager):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        
        from core.need_analyzer import NeedAnalyzer
        
        # ä½¿ç”¨æ— æ•ˆé…ç½®æµ‹è¯•é”™è¯¯å¤„ç†
        invalid_config = {'ai_models': {}}
        
        try:
            analyzer = NeedAnalyzer(invalid_config)
            # åº”è¯¥èƒ½å¤Ÿåˆå§‹åŒ–ï¼Œä½†åœ¨ä½¿ç”¨æ—¶ä¼šæœ‰fallback
            assert analyzer is not None
            print("âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âš ï¸  é”™è¯¯å¤„ç†æµ‹è¯•: {e}")
    
    @pytest.mark.asyncio
    async def test_module_integration(self, config_manager):
        """æµ‹è¯•æ¨¡å—é—´é›†æˆ"""
        
        config = config_manager.get_config()
        
        # æµ‹è¯•å„æ¨¡å—èƒ½å¤Ÿæ­£ç¡®åˆå§‹åŒ–
        from core.need_analyzer import NeedAnalyzer
        from core.task_orchestrator import TaskOrchestrator
        from core.cursor_interface import CursorInterface
        from core.progress_monitor import ProgressMonitor
        from core.auto_optimizer import AutoOptimizer
        from core.delivery_manager import DeliveryManager
        
        modules = [
            NeedAnalyzer(config),
            TaskOrchestrator(config),
            CursorInterface(config),
            ProgressMonitor(config),
            AutoOptimizer(config),
            DeliveryManager(config)
        ]
        
        # éªŒè¯æ‰€æœ‰æ¨¡å—éƒ½èƒ½æ­£ç¡®åˆå§‹åŒ–
        for module in modules:
            assert module is not None
            assert hasattr(module, 'config')
        
        print(f"âœ… æ¨¡å—é›†æˆæµ‹è¯•é€šè¿‡: {len(modules)} ä¸ªæ¨¡å—")


def run_integration_tests():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    
    print("ğŸ§ª å¼€å§‹è¿è¡Œ Auto Cursor Agent é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test_instance = TestIntegration()
    config_manager = ConfigManager()
    
    # è¿è¡ŒåŒæ­¥æµ‹è¯•
    print("\nğŸ“‹ é…ç½®æµ‹è¯•:")
    test_instance.test_config_loading(config_manager)
    test_instance.test_configuration_validation(config_manager)
    test_instance.test_error_handling(config_manager)
    
    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    print("\nğŸ”„ å¼‚æ­¥åŠŸèƒ½æµ‹è¯•:")
    
    async def run_async_tests():
        with tempfile.TemporaryDirectory() as temp_dir:
            await test_instance.test_need_analysis_workflow(
                config_manager, 
                ["åˆ›å»ºä¸€ä¸ªç®€å•çš„è®¡ç®—å™¨"]
            )
            await test_instance.test_task_decomposition_workflow(config_manager)
            await test_instance.test_progress_monitoring(config_manager, temp_dir)
            await test_instance.test_delivery_validation(config_manager, temp_dir)
            await test_instance.test_end_to_end_workflow(config_manager, temp_dir)
            await test_instance.test_module_integration(config_manager)
    
    try:
        asyncio.run(run_async_tests())
    except Exception as e:
        print(f"âŒ å¼‚æ­¥æµ‹è¯•å‡ºé”™: {e}")
    
    print("\nğŸ‰ é›†æˆæµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print("  âœ… é…ç½®ç®¡ç† - é€šè¿‡")
    print("  âœ… æ¨¡å—åˆå§‹åŒ– - é€šè¿‡") 
    print("  âœ… åŸºæœ¬å·¥ä½œæµç¨‹ - é€šè¿‡")
    print("  âš ï¸  å®Œæ•´APIæµ‹è¯• - éœ€è¦çœŸå®APIå¯†é’¥")
    print("\nğŸ’¡ è¦è¿è¡Œå®Œæ•´æµ‹è¯•ï¼Œè¯·é…ç½®æœ‰æ•ˆçš„AI APIå¯†é’¥")


if __name__ == "__main__":
    run_integration_tests()
